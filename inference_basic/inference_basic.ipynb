{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyO8AgejQ9M5noS/5ls2jotU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JackCaoG/torch-xla-examples/blob/main/inference_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0KjvAxCsSlD",
        "outputId": "18a7cbd6-13d9-4a33-97bd-725d2f00b810"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3.0+cpu\n",
            "2.3.0+libtpu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# This must be set before importing `torch_xla`, turn this on only for debugging.\n",
        "# For more debugging tips, check https://youtu.be/LK3A3vjo-KQ?si=_7tH7pvFIDJH7NWM\n",
        "os.environ['PT_XLA_DEBUG'] = \"1\"\n",
        "os.environ['PT_XLA_DEBUG_FILE'] = \"/tmp/pt_xla_debug.txt\"\n",
        "\n",
        "import torch\n",
        "import torch_xla\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.debug.metrics as met\n",
        "\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torch_xla.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch_xla.device()\n",
        "\n",
        "\n",
        "# dummy inference example, you can replace linear with your model\n",
        "linear = nn.Linear(10, 20).to(device)\n",
        "input = torch.randn(1, 10).to(device)"
      ],
      "metadata": {
        "id": "iA9LhNcrsvG2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clear the metrics before executation if you only care about metrics\n",
        "# for the current execution.\n",
        "met.clear_all()\n",
        "\n",
        "def single_step_linear(input):\n",
        "  # no grad is optional, but it is a good practice for inference\n",
        "  with torch.no_grad():\n",
        "    res = linear(input)\n",
        "  # trigger the execution\n",
        "  xm.mark_step()\n",
        "  return res\n",
        "\n",
        "res = single_step_linear(input)\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNyIqrQbwzrH",
        "outputId": "b92a6db1-cc0d-4068-eb4a-3190e1541ebe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.9603, -0.2537, -1.2938,  0.2158,  0.4935, -0.2579,  0.2074, -0.3205,\n",
            "          0.8562, -0.5789,  0.9714, -0.4794, -0.3721,  0.4322,  0.1533, -0.3180,\n",
            "         -0.1307, -0.4394, -0.5698, -0.8652]], device='xla:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Things to check for an efficent inference (and also training)\n",
        "# 1. There is one execution per step\n",
        "#    1.1 no op fallback to CPU\n",
        "#    1.2 no dynamic shape in both input shape and model code\n",
        "\n",
        "\n",
        "# For 1, check the metrics `ExecuteTime` and `CompileTime`.\n",
        "# You can also check the files `/tmp/pt_xla_debug.txt` we specified above and\n",
        "# make sure there is only one `Execution` output for a single step.\n",
        "print(met.short_metrics_report())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZu0ghmMtjgK",
        "outputId": "4fa3853f-5cfe-4e15-e61e-f96de998ddbf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metric: CompileTime\n",
            "  TotalSamples: 1\n",
            "  Accumulator: 324ms435.056us\n",
            "  Percentiles: 1%=324ms435.056us; 5%=324ms435.056us; 10%=324ms435.056us; 20%=324ms435.056us; 50%=324ms435.056us; 80%=324ms435.056us; 90%=324ms435.056us; 95%=324ms435.056us; 99%=324ms435.056us\n",
            "Metric: ExecuteTime\n",
            "  TotalSamples: 1\n",
            "  Accumulator: 007ms849.159us\n",
            "  Percentiles: 1%=007ms849.159us; 5%=007ms849.159us; 10%=007ms849.159us; 20%=007ms849.159us; 50%=007ms849.159us; 80%=007ms849.159us; 90%=007ms849.159us; 95%=007ms849.159us; 99%=007ms849.159us\n",
            "Metric: TransferFromDeviceTime\n",
            "  TotalSamples: 1\n",
            "  Accumulator: 004ms551.870us\n",
            "  Percentiles: 1%=004ms551.870us; 5%=004ms551.870us; 10%=004ms551.870us; 20%=004ms551.870us; 50%=004ms551.870us; 80%=004ms551.870us; 90%=004ms551.870us; 95%=004ms551.870us; 99%=004ms551.870us\n",
            "Counter: MarkStep\n",
            "  Value: 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For 1.1, check there is no `aten::` metrics\n",
        "met.clear_all()\n",
        "fallback_tensor = torch.nonzero(input)\n",
        "xm.mark_step()\n",
        "\n",
        "# wait for all async ops to finish\n",
        "xm.wait_device_ops()\n",
        "# If you see any `aten::` metrics, open a bug to PyTorch/XLA github and\n",
        "# we will try to lower it.\n",
        "print(met.short_metrics_report())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YwBqou4uG4E",
        "outputId": "35804bb8-67a5-4cbd-c464-97534ed455bc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metric: CompileTime\n",
            "  TotalSamples: 1\n",
            "  Accumulator: 021ms654.651us\n",
            "  Percentiles: 1%=021ms654.651us; 5%=021ms654.651us; 10%=021ms654.651us; 20%=021ms654.651us; 50%=021ms654.651us; 80%=021ms654.651us; 90%=021ms654.651us; 95%=021ms654.651us; 99%=021ms654.651us\n",
            "Metric: ExecuteTime\n",
            "  TotalSamples: 1\n",
            "  Accumulator: 002ms673.154us\n",
            "  Percentiles: 1%=002ms673.154us; 5%=002ms673.154us; 10%=002ms673.154us; 20%=002ms673.154us; 50%=002ms673.154us; 80%=002ms673.154us; 90%=002ms673.154us; 95%=002ms673.154us; 99%=002ms673.154us\n",
            "Metric: TransferToDeviceTime\n",
            "  TotalSamples: 1\n",
            "  Accumulator: 058.402us\n",
            "  Percentiles: 1%=058.402us; 5%=058.402us; 10%=058.402us; 20%=058.402us; 50%=058.402us; 80%=058.402us; 90%=058.402us; 95%=058.402us; 99%=058.402us\n",
            "Metric: TransferFromDeviceTime\n",
            "  TotalSamples: 1\n",
            "  Accumulator: 569.034us\n",
            "  Percentiles: 1%=569.034us; 5%=569.034us; 10%=569.034us; 20%=569.034us; 50%=569.034us; 80%=569.034us; 90%=569.034us; 95%=569.034us; 99%=569.034us\n",
            "Counter: MarkStep\n",
            "  Value: 1\n",
            "Counter: aten::nonzero\n",
            "  Value: 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For 1.2, run your model x steps and make sure it does not recompile\n",
        "met.clear_all()\n",
        "\n",
        "for _ in range(10):\n",
        "  input = torch.randn(1, 10).to(device)\n",
        "  res = single_step_linear(input)\n",
        "\n",
        "# wait for all async ops to finish\n",
        "xm.wait_device_ops()\n",
        "print(met.short_metrics_report())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0G7xKCA_xujx",
        "outputId": "dc6825a3-0727-42e7-89f9-1596beb6313c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter: CachedCompile\n",
            "  Value: 9\n",
            "Metric: CompileTime\n",
            "  TotalSamples: 1\n",
            "  Accumulator: 026ms647.611us\n",
            "  Percentiles: 1%=026ms647.611us; 5%=026ms647.611us; 10%=026ms647.611us; 20%=026ms647.611us; 50%=026ms647.611us; 80%=026ms647.611us; 90%=026ms647.611us; 95%=026ms647.611us; 99%=026ms647.611us\n",
            "Metric: ExecuteTime\n",
            "  TotalSamples: 10\n",
            "  Accumulator: 030ms336.955us\n",
            "  ValueRate: 02s196ms079.568us / second\n",
            "  Rate: 723.896 / second\n",
            "  Percentiles: 1%=433.567us; 5%=433.567us; 10%=502.263us; 20%=752.633us; 50%=001ms462.685us; 80%=008ms936.260us; 90%=009ms745.577us; 95%=009ms745.577us; 99%=009ms745.577us\n",
            "Metric: TransferToDeviceTime\n",
            "  TotalSamples: 10\n",
            "  Accumulator: 506.849us\n",
            "  ValueRate: 011ms641.075us / second\n",
            "  Rate: 209.946 / second\n",
            "  Percentiles: 1%=034.891us; 5%=034.891us; 10%=037.006us; 20%=037.275us; 50%=048.435us; 80%=054.783us; 90%=104.311us; 95%=104.311us; 99%=104.311us\n",
            "Counter: MarkStep\n",
            "  Value: 10\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If your model is compiling for every execution and the input shape is fixed\n",
        "# it is likely that your model has some dynamism in it. Try to follow\n",
        "# https://github.com/pytorch/xla/blob/master/TROUBLESHOOTING.md#common-debugging-environment-variables-combinations\n",
        "# to dump the IR or HLO and compare it across runs and see where the dynamism coming from."
      ],
      "metadata": {
        "id": "gD8bB8I1i_bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lastly use `torch.compile` to speed up your model inference\n",
        "compiled_linear = torch.compile(linear, backend=\"openxla\")\n",
        "\n",
        "met.clear_all()\n",
        "\n",
        "with torch.no_grad():\n",
        "  # you don't need mark_step if model is compiled\n",
        "  res = compiled_linear(input)\n",
        "\n",
        "# wait for all async ops to finish\n",
        "xm.wait_device_ops()\n",
        "print(met.short_metrics_report())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAHiTnJo1LiU",
        "outputId": "ae946abb-decb-459e-86ac-36ea43bf9c7e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metric: CompileTime\n",
            "  TotalSamples: 1\n",
            "  Accumulator: 026ms185.527us\n",
            "  Percentiles: 1%=026ms185.527us; 5%=026ms185.527us; 10%=026ms185.527us; 20%=026ms185.527us; 50%=026ms185.527us; 80%=026ms185.527us; 90%=026ms185.527us; 95%=026ms185.527us; 99%=026ms185.527us\n",
            "Metric: ExecuteTime\n",
            "  TotalSamples: 1\n",
            "  Accumulator: 006ms484.284us\n",
            "  Percentiles: 1%=006ms484.284us; 5%=006ms484.284us; 10%=006ms484.284us; 20%=006ms484.284us; 50%=006ms484.284us; 80%=006ms484.284us; 90%=006ms484.284us; 95%=006ms484.284us; 99%=006ms484.284us\n",
            "Counter: MarkStep\n",
            "  Value: 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now try to inference a slightly more compilated model\n",
        "import torchvision\n",
        "\n",
        "model = torchvision.models.resnet18().to(device)\n",
        "compiled_model = torch.compile(model, backend=\"openxla\")\n",
        "\n",
        "# [Batch, Channel, dim, dim]\n",
        "resnet_input = torch.randn(64, 3, 224, 224).to(device)"
      ],
      "metadata": {
        "id": "rujIxNQ1TFcm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "met.clear_all()\n",
        "for _ in range(10):\n",
        "  res = compiled_model(resnet_input)\n",
        "\n",
        "# wait for all async ops to finish\n",
        "xm.wait_device_ops()\n",
        "print(met.short_metrics_report())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uftsfrAIUalF",
        "outputId": "8e64a27e-ac1c-4c79-cea4-fbdf5903bc7c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metric: ExecuteTime\n",
            "  TotalSamples: 10\n",
            "  Accumulator: 01s451ms058.179us\n",
            "  ValueRate: 03s193ms215.897us / second\n",
            "  Rate: 22.0061 / second\n",
            "  Percentiles: 1%=053ms988.449us; 5%=053ms988.449us; 10%=101ms992.691us; 20%=149ms625.897us; 50%=150ms026.367us; 80%=197ms511.857us; 90%=201ms579.853us; 95%=201ms579.853us; 99%=201ms579.853us\n",
            "\n"
          ]
        }
      ]
    }
  ]
}