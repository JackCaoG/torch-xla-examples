{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyPWvYd9F5+Fzjx/SyYAppbp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JackCaoG/torch-xla-examples/blob/main/spmd_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqqJBHeWkd1v",
        "outputId": "b333ffa3-01c2-4775-8200-3111f4b5f7fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3.0+cpu\n",
            "2.3.0+libtpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch_xla\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch_xla.runtime as xr\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.spmd as xs\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torch_xla.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable XLA SPMD execution mode.\n",
        "xr.use_spmd()"
      ],
      "metadata": {
        "id": "PdGfvDeKlD2S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0199c4c-5036-469e-ecc0-d80365c182f6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_xla/runtime.py:242: UserWarning: XLA_USE_SPMD is being deprecated. Use torch_xla.runtime.use_spmd() without setting XLA_USE_SPMD env-var.\n",
            "  warnings.warn(\"XLA_USE_SPMD is being deprecated. \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Device mesh, this and partition spec as well as the input tensor shape define the individual shard shape.\n",
        "num_devices = xr.global_runtime_device_count()\n",
        "mesh_shape = (num_devices, 1)\n",
        "device_ids = np.array(range(num_devices))\n",
        "mesh = xs.Mesh(device_ids, mesh_shape, ('data', 'model'))"
      ],
      "metadata": {
        "id": "c2zipPxvlHlW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mesh.axis_names)\n",
        "print(mesh.mesh_shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvEBeX-nlbdJ",
        "outputId": "9995c464-cd22-49fe-86aa-5922833be2aa"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('data', 'model')\n",
            "(8, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t_cpu = torch.randn(8, 4)\n",
        "t_xla = t_cpu.to(torch_xla.device())"
      ],
      "metadata": {
        "id": "Ttf56lKDma5b"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mesh partitioning, each device holds 1/8-th of the input\n",
        "partition_spec = ('data', 'model')\n",
        "xt = xs.mark_sharding(t_xla, mesh, partition_spec)\n",
        "print(len(xt.local_shards))\n",
        "print(xt.local_shards[0])\n",
        "print(xt.local_shards[1])\n",
        "print(t_cpu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1WuVi5QlnHY",
        "outputId": "54ac51b7-4964-4b0a-8203-b2960e68067b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n",
            "XLAShard(data=tensor([[ 1.1481,  1.0661, -0.4202, -0.7410]]), indices=[slice(0, 1, 1), slice(0, 4, 1)], shard_device='TPU:0', replica_id=0)\n",
            "XLAShard(data=tensor([[ 0.2942,  0.3965, -0.2677, -0.8216]]), indices=[slice(1, 2, 1), slice(0, 4, 1)], shard_device='TPU:1', replica_id=0)\n",
            "tensor([[ 1.1481,  1.0661, -0.4202, -0.7410],\n",
            "        [ 0.2942,  0.3965, -0.2677, -0.8216],\n",
            "        [ 0.9702, -1.0988, -0.8096,  0.3651],\n",
            "        [ 0.1583, -1.0259,  0.5243, -1.5866],\n",
            "        [ 1.7533,  1.9341, -0.1616,  0.2106],\n",
            "        [ 0.6075, -0.3239, -0.7455,  0.7426],\n",
            "        [-0.7109, -1.8223,  1.3984,  1.6798],\n",
            "        [-0.7604, -2.1333,  0.7389, -0.4245]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(t_xla))\n",
        "print(t_xla.device)\n",
        "print(type(xt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpfKt-0AnlPA",
        "outputId": "5727e4e8-8c10-4305-9c15-1c43ca7a597e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n",
            "xla:0\n",
            "<class 'torch_xla.distributed.spmd.xla_sharded_tensor.XLAShardedTensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.allclose(torch.cos(t_xla).cpu(), torch.cos(t_cpu))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsETLYxFm625",
        "outputId": "feb1eeaf-de00-4e30-a45d-2fefd33c2ab8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "\n",
        "device = torch_xla.device()\n",
        "\n",
        "model = torchvision.models.resnet18().to(device)\n",
        "# [Batch, Channel, dim, dim]\n",
        "input = torch.randn(512, 3, 224, 224).to(device)\n",
        "\n",
        "# Shard at batch dimension, this is data parallel.\n",
        "xs.mark_sharding(input, mesh, ('data', None, None, None))\n",
        "\n",
        "loss = model(input)\n",
        "xm.mark_step()\n",
        "\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITfo0FLeny3W",
        "outputId": "c61f9eab-eda0-4f35-d756-9e37798e6cdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.3702, -0.3322,  0.7581,  ...,  0.1561, -0.4892,  0.2370],\n",
            "        [-0.4208, -0.2955,  0.7115,  ...,  0.1580, -0.7177,  0.0763],\n",
            "        [-0.4375, -0.3585,  0.7511,  ..., -0.0321, -0.7185,  0.2437],\n",
            "        ...,\n",
            "        [-0.5015, -0.4109,  0.7333,  ...,  0.2299, -0.7468,  0.2354],\n",
            "        [-0.4877, -0.2055,  0.7506,  ...,  0.0622, -0.8617,  0.1749],\n",
            "        [-0.3511, -0.3073,  0.7137,  ...,  0.0111, -0.6867,  0.2517]],\n",
            "       device='xla:0', grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    }
  ]
}